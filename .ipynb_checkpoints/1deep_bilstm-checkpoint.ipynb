{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "torch.set_num_threads(8)\n",
    "import sys\n",
    "import codecs\n",
    "import random\n",
    "import torch.utils.data as Data\n",
    "\n",
    "\n",
    "SEED = 1\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# input: a sequence of tokens, and a token_to_index dictionary\n",
    "# output: a LongTensor variable to encode the sequence of idxs\n",
    "def prepare_sequence(seq, to_ix, cuda=False):\n",
    "    var = autograd.Variable(torch.LongTensor([to_ix[w] for w in seq.split(' ')]))\n",
    "    return var\n",
    "\n",
    "def prepare_label(label,label_to_ix, cuda=False):\n",
    "    var = autograd.Variable(torch.LongTensor([label_to_ix[label]]))\n",
    "    return var\n",
    "\n",
    "def build_token_to_ix(sentences):\n",
    "    token_to_ix = dict()\n",
    "    print(len(sentences))\n",
    "    for sent in sentences:\n",
    "        for token in sent.split(' '):\n",
    "            if token not in token_to_ix:\n",
    "                token_to_ix[token] = len(token_to_ix)\n",
    "    token_to_ix['<pad>'] = len(token_to_ix)\n",
    "    return token_to_ix\n",
    "\n",
    "def build_label_to_ix(labels):\n",
    "    label_to_ix = dict()\n",
    "    for label in labels:\n",
    "        if label not in label_to_ix:\n",
    "            label_to_ix[label] = len(label_to_ix)\n",
    "\n",
    "\n",
    "def get_all_files_from_dir(dirpath):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    onlyfiles = [f for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "    return onlyfiles\n",
    "\n",
    "def head_n(fname, n=2):\n",
    "    count = 1\n",
    "    print '---------------------------------------'\n",
    "    f = open(fname)\n",
    "    for line in f:\n",
    "        print line\n",
    "        count += 1\n",
    "        if count > n:\n",
    "            print '-----------------------------------'\n",
    "            f.close()\n",
    "            return\n",
    "#head_n(test_file_pos,10)\n",
    "#thos function would only extract vp and vn files, in order to extract vpn = vp+vn in one file, give last arguement as False/0 \n",
    "\n",
    "def extract_names(l_files,patt,p_xor_n = True):\n",
    "    # note that you may need to \n",
    "    r = []\n",
    "    for f in l_files:\n",
    "        tokens = f.split(\".\")\n",
    "        if tokens[-3]==patt:\n",
    "            if p_xor_n:\n",
    "                if tokens[-2] != 'vpn':\n",
    "                    r.append(f)\n",
    "                    #yield f\n",
    "            elif tokens[-2] == 'vpn':\n",
    "                return f\n",
    "    return sorted(r)\n",
    "#extract_names(files,'test')\n",
    "def get_sentence_out(path):\n",
    "    f = open(path)\n",
    "    return map(lambda x:x.split(\",\")[2],f)\n",
    "def get_neg_pos_sent(type_,files,path):\n",
    "    return [get_sentence_out(path+n) for n in extract_names(files,type_)]\n",
    "def load_stanford_data():\n",
    "    fpath = './cross_validation_data/vpn_filtered/'\n",
    "    files = get_all_files_from_dir(fpath)\n",
    "    \n",
    "    train_sent_neg,train_sent_pos = get_neg_pos_sent('train',files,fpath)\n",
    "    val_sent_neg,val_sent_pos = get_neg_pos_sent('dev',files,fpath)\n",
    "    test_sent_neg,test_sent_pos = get_neg_pos_sent('test',files,fpath)\n",
    "    \n",
    "    train_data = [(sent,1) for sent in train_sent_pos] + [(sent, 0) for sent in train_sent_neg]\n",
    "    dev_data = [(sent, 1) for sent in val_sent_pos] + [(sent, 0) for sent in val_sent_neg]\n",
    "    test_data = [(sent, 1) for sent in test_sent_pos] + [(sent, 0) for sent in test_sent_neg]\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "    random.shuffle(dev_data)\n",
    "    random.shuffle(test_data)\n",
    "\n",
    "    print('train:',len(train_data),'dev:',len(dev_data),'test:',len(test_data))\n",
    "    \n",
    "    word_to_ix = build_token_to_ix([s for s,_ in train_data+dev_data+test_data])\n",
    "    label_to_ix = {0:0,1:1}\n",
    "    print('vocab size:',len(word_to_ix),'label size:',len(label_to_ix))\n",
    "    print('loading data done!')\n",
    "    return train_data,dev_data,test_data,word_to_ix,label_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train:', 25667, 'dev:', 292, 'test:', 657)\n",
      "26616\n",
      "('vocab size:', 14378, 'label size:', 2)\n",
      "loading data done!\n"
     ]
    }
   ],
   "source": [
    "# Load the data and define hyperparametars and specifications ###########################\n",
    "train_data, dev_data, test_data, word_to_ix, label_to_ix = load_stanford_data()\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "EPOCH = 20\n",
    "NUM_LAYERS = 1\n",
    "loss_function = nn.NLLLoss()#negative log likelihood loss \n",
    "########################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 start!\n",
      "epoch: 0 iterations: 500 loss :0.838363\n",
      "epoch: 0 iterations: 1000 loss :0.831246\n",
      "epoch: 0 iterations: 1500 loss :0.313535\n",
      "epoch: 0 iterations: 2000 loss :0.664001\n",
      "epoch: 0 iterations: 2500 loss :0.305257\n",
      "epoch: 0 iterations: 3000 loss :0.57258\n",
      "epoch: 0 iterations: 3500 loss :0.263908\n",
      "epoch: 0 iterations: 4000 loss :0.13738\n",
      "epoch: 0 iterations: 4500 loss :0.173922\n",
      "epoch: 0 iterations: 5000 loss :0.158149\n",
      "epoch: 0 iterations: 5500 loss :0.0439997\n",
      "epoch: 0 iterations: 6000 loss :0.338978\n",
      "epoch: 0 iterations: 6500 loss :0.0299123\n",
      "epoch: 0 iterations: 7000 loss :0.15347\n",
      "epoch: 0 iterations: 7500 loss :0.0825118\n",
      "epoch: 0 iterations: 8000 loss :0.217841\n",
      "epoch: 0 iterations: 8500 loss :1.9849\n",
      "epoch: 0 iterations: 9000 loss :0.00330346\n",
      "epoch: 0 iterations: 9500 loss :0.199486\n",
      "epoch: 0 iterations: 10000 loss :0.17489\n",
      "epoch: 0 iterations: 10500 loss :1.25033\n",
      "epoch: 0 iterations: 11000 loss :0.0751075\n",
      "epoch: 0 iterations: 11500 loss :0.0245517\n",
      "epoch: 0 iterations: 12000 loss :0.0996375\n",
      "epoch: 0 iterations: 12500 loss :0.00500719\n",
      "epoch: 0 iterations: 13000 loss :0.013502\n",
      "epoch: 0 iterations: 13500 loss :0.027626\n",
      "epoch: 0 iterations: 14000 loss :2.29205\n",
      "epoch: 0 iterations: 14500 loss :0.013458\n",
      "epoch: 0 iterations: 15000 loss :0.133592\n",
      "epoch: 0 iterations: 15500 loss :0.00888546\n",
      "epoch: 0 iterations: 16000 loss :0.00500598\n",
      "epoch: 0 iterations: 16500 loss :0.0981435\n",
      "epoch: 0 iterations: 17000 loss :0.0821122\n",
      "epoch: 0 iterations: 17500 loss :0.0176901\n",
      "epoch: 0 iterations: 18000 loss :0.138857\n",
      "epoch: 0 iterations: 18500 loss :0.00783559\n",
      "epoch: 0 iterations: 19000 loss :0.00170822\n",
      "epoch: 0 iterations: 19500 loss :0.0161285\n",
      "epoch: 0 iterations: 20000 loss :0.0074525\n",
      "epoch: 0 iterations: 20500 loss :0.772158\n",
      "epoch: 0 iterations: 21000 loss :0.0073122\n",
      "epoch: 0 iterations: 21500 loss :0.521592\n",
      "epoch: 0 iterations: 22000 loss :0.0168758\n",
      "epoch: 0 iterations: 22500 loss :0.000994931\n",
      "epoch: 0 iterations: 23000 loss :2.20651\n",
      "epoch: 0 iterations: 23500 loss :0.0028414\n",
      "epoch: 0 iterations: 24000 loss :0.0339135\n",
      "epoch: 0 iterations: 24500 loss :0.00146965\n",
      "epoch: 0 iterations: 25000 loss :0.00367901\n",
      "epoch: 0 iterations: 25500 loss :0.025564\n",
      "epoch: 0 done! \n",
      " train avg_loss:0.31727 , acc:0.852963\n",
      "('now best dev acc:', 0.0)\n",
      "dev avg_loss:0.247924 train acc:0.928082\n",
      "test avg_loss:0.153829 train acc:0.931507\n",
      "New Best Dev!!!\n",
      "epoch: 1 start!\n",
      "epoch: 1 iterations: 500 loss :0.0727104\n",
      "epoch: 1 iterations: 1000 loss :6.84269e-05\n",
      "epoch: 1 iterations: 1500 loss :3.26325e-05\n",
      "epoch: 1 iterations: 2000 loss :0.00198248\n",
      "epoch: 1 iterations: 2500 loss :0.00591973\n",
      "epoch: 1 iterations: 3000 loss :0.000730452\n",
      "epoch: 1 iterations: 3500 loss :0.00721307\n",
      "epoch: 1 iterations: 4000 loss :7.41179e-05\n",
      "epoch: 1 iterations: 4500 loss :0.000424854\n",
      "epoch: 1 iterations: 5000 loss :0.000345154\n",
      "epoch: 1 iterations: 5500 loss :0.000730085\n",
      "epoch: 1 iterations: 6000 loss :0.0773346\n",
      "epoch: 1 iterations: 6500 loss :0.00193604\n",
      "epoch: 1 iterations: 7000 loss :0.000106675\n",
      "epoch: 1 iterations: 7500 loss :0.00847136\n",
      "epoch: 1 iterations: 8000 loss :0.00210222\n",
      "epoch: 1 iterations: 8500 loss :3.44707e-06\n",
      "epoch: 1 iterations: 9000 loss :0.000203402\n",
      "epoch: 1 iterations: 9500 loss :0.416092\n",
      "epoch: 1 iterations: 10000 loss :0.0137445\n",
      "epoch: 1 iterations: 10500 loss :0.0315519\n",
      "epoch: 1 iterations: 11000 loss :0.00021322\n",
      "epoch: 1 iterations: 11500 loss :0.00214962\n",
      "epoch: 1 iterations: 12000 loss :0.00059202\n",
      "epoch: 1 iterations: 12500 loss :0.00436267\n",
      "epoch: 1 iterations: 13000 loss :2.58216e-05\n",
      "epoch: 1 iterations: 13500 loss :0.0146021\n",
      "epoch: 1 iterations: 14000 loss :0.00411381\n",
      "epoch: 1 iterations: 14500 loss :3.35742e-05\n",
      "epoch: 1 iterations: 15000 loss :0.00242797\n",
      "epoch: 1 iterations: 15500 loss :0.00205429\n",
      "epoch: 1 iterations: 16000 loss :4.00722e-05\n",
      "epoch: 1 iterations: 16500 loss :0.098382\n",
      "epoch: 1 iterations: 17000 loss :0.0213551\n",
      "epoch: 1 iterations: 17500 loss :0.0150632\n",
      "epoch: 1 iterations: 18000 loss :0.00175328\n",
      "epoch: 1 iterations: 18500 loss :0.00136218\n",
      "epoch: 1 iterations: 19000 loss :0.0208715\n",
      "epoch: 1 iterations: 19500 loss :0.00503649\n",
      "epoch: 1 iterations: 20000 loss :0.00634645\n",
      "epoch: 1 iterations: 20500 loss :0.00331941\n",
      "epoch: 1 iterations: 21000 loss :0.00033239\n",
      "epoch: 1 iterations: 21500 loss :0.0212787\n",
      "epoch: 1 iterations: 22000 loss :0.00221061\n",
      "epoch: 1 iterations: 22500 loss :0.000284894\n",
      "epoch: 1 iterations: 23000 loss :0.000951994\n",
      "epoch: 1 iterations: 23500 loss :0.00034918\n",
      "epoch: 1 iterations: 24000 loss :0.000374051\n",
      "epoch: 1 iterations: 24500 loss :0.00342791\n",
      "epoch: 1 iterations: 25000 loss :0.747896\n",
      "epoch: 1 iterations: 25500 loss :0.00136867\n",
      "epoch: 1 done! \n",
      " train avg_loss:0.0917297 , acc:0.967351\n",
      "('now best dev acc:', 0.928082191780822)\n",
      "dev avg_loss:0.200614 train acc:0.94863\n",
      "test avg_loss:0.12504 train acc:0.957382\n",
      "New Best Dev!!!\n",
      "epoch: 2 start!\n",
      "epoch: 2 iterations: 500 loss :0.00184574\n",
      "epoch: 2 iterations: 1000 loss :3.83739e-05\n",
      "epoch: 2 iterations: 1500 loss :0.000192342\n",
      "epoch: 2 iterations: 2000 loss :0.00109516\n",
      "epoch: 2 iterations: 2500 loss :0.00339395\n",
      "epoch: 2 iterations: 3000 loss :4.22949e-05\n",
      "epoch: 2 iterations: 3500 loss :3.10832e-07\n",
      "epoch: 2 iterations: 4000 loss :3.71676e-05\n",
      "epoch: 2 iterations: 4500 loss :0.0102736\n",
      "epoch: 2 iterations: 5000 loss :0.000811026\n",
      "epoch: 2 iterations: 5500 loss :4.10562e-06\n",
      "epoch: 2 iterations: 6000 loss :0.000651405\n",
      "epoch: 2 iterations: 6500 loss :2.84863e-05\n",
      "epoch: 2 iterations: 7000 loss :1.53958e-05\n",
      "epoch: 2 iterations: 7500 loss :1.80651e-05\n",
      "epoch: 2 iterations: 8000 loss :0.657296\n",
      "epoch: 2 iterations: 8500 loss :8.55816e-06\n",
      "epoch: 2 iterations: 9000 loss :0.0102004\n",
      "epoch: 2 iterations: 9500 loss :1.15635e-05\n",
      "epoch: 2 iterations: 10000 loss :4.86475e-06\n",
      "epoch: 2 iterations: 10500 loss :3.5145e-06\n",
      "epoch: 2 iterations: 11000 loss :7.93831e-08\n",
      "epoch: 2 iterations: 11500 loss :0.00141046\n",
      "epoch: 2 iterations: 12000 loss :0.00996473\n",
      "epoch: 2 iterations: 12500 loss :1.43039e-05\n",
      "epoch: 2 iterations: 13000 loss :3.8869e-05\n",
      "epoch: 2 iterations: 13500 loss :0.161478\n",
      "epoch: 2 iterations: 14000 loss :0.00445628\n",
      "epoch: 2 iterations: 14500 loss :0.00887451\n",
      "epoch: 2 iterations: 15000 loss :0.00171597\n",
      "epoch: 2 iterations: 15500 loss :1.38275e-06\n",
      "epoch: 2 iterations: 16000 loss :7.07189e-06\n",
      "epoch: 2 iterations: 16500 loss :0.020228\n",
      "epoch: 2 iterations: 17000 loss :0.000182623\n",
      "epoch: 2 iterations: 17500 loss :1.15363e-05\n",
      "epoch: 2 iterations: 18000 loss :6.34671e-05\n",
      "epoch: 2 iterations: 18500 loss :0.00108174\n",
      "epoch: 2 iterations: 19000 loss :2.96428e-05\n",
      "epoch: 2 iterations: 19500 loss :0.00016115\n",
      "epoch: 2 iterations: 20000 loss :0.00267448\n",
      "epoch: 2 iterations: 20500 loss :5.97379e-05\n",
      "epoch: 2 iterations: 21000 loss :1.88373e-07\n",
      "epoch: 2 iterations: 21500 loss :4.48115e-06\n",
      "epoch: 2 iterations: 22000 loss :2.62429e-07\n",
      "epoch: 2 iterations: 22500 loss :5.68164e-08\n",
      "epoch: 2 iterations: 23000 loss :0.000146013\n",
      "epoch: 2 iterations: 23500 loss :0.000636533\n",
      "epoch: 2 iterations: 24000 loss :0.000308983\n",
      "epoch: 2 iterations: 24500 loss :6.19627e-06\n",
      "epoch: 2 iterations: 25000 loss :2.56847e-06\n",
      "epoch: 2 iterations: 25500 loss :9.71875e-06\n",
      "epoch: 2 done! \n",
      " train avg_loss:0.0365408 , acc:0.986948\n",
      "('now best dev acc:', 0.9486301369863014)\n",
      "dev avg_loss:0.250294 train acc:0.94863\n",
      "test avg_loss:0.174486 train acc:0.951294\n",
      "epoch: 3 start!\n",
      "epoch: 3 iterations: 500 loss :0.000382148\n",
      "epoch: 3 iterations: 1000 loss :4.90689e-05\n",
      "epoch: 3 iterations: 1500 loss :6.67025e-06\n",
      "epoch: 3 iterations: 2000 loss :6.50241e-07\n",
      "epoch: 3 iterations: 2500 loss :1.70164e-05\n",
      "epoch: 3 iterations: 3000 loss :5.76145e-06\n",
      "epoch: 3 iterations: 3500 loss :3.15413e-07\n",
      "epoch: 3 iterations: 4000 loss :0.000193216\n",
      "epoch: 3 iterations: 4500 loss :3.34843e-07\n",
      "epoch: 3 iterations: 5000 loss :0.00012269\n",
      "epoch: 3 iterations: 5500 loss :1.18428e-06\n",
      "epoch: 3 iterations: 6000 loss :1.24466e-05\n",
      "epoch: 3 iterations: 6500 loss :0.000341224\n",
      "epoch: 3 iterations: 7000 loss :5.76516e-06\n",
      "epoch: 3 iterations: 7500 loss :9.50947e-06\n",
      "epoch: 3 iterations: 8000 loss :1.09295e-06\n",
      "epoch: 3 iterations: 8500 loss :1.86393e-07\n",
      "epoch: 3 iterations: 9000 loss :1.39818e-07\n",
      "epoch: 3 iterations: 9500 loss :9.53963e-08\n",
      "epoch: 3 iterations: 10000 loss :3.87562e-07\n",
      "epoch: 3 iterations: 10500 loss :4.9829e-06\n",
      "epoch: 3 iterations: 11000 loss :4.41179e-08\n",
      "epoch: 3 iterations: 11500 loss :2.4789e-07\n",
      "epoch: 3 iterations: 12000 loss :2.69582e-08\n",
      "epoch: 3 iterations: 12500 loss :2.56544e-08\n",
      "epoch: 3 iterations: 13000 loss :1.49342e-06\n",
      "epoch: 3 iterations: 13500 loss :0.000310643\n",
      "epoch: 3 iterations: 14000 loss :3.30675e-05\n",
      "epoch: 3 iterations: 14500 loss :4.7401e-06\n",
      "epoch: 3 iterations: 15000 loss :4.5252e-07\n",
      "epoch: 3 iterations: 15500 loss :1.76413e-06\n",
      "epoch: 3 iterations: 16000 loss :5.93448e-05\n",
      "epoch: 3 iterations: 16500 loss :3.05982e-09\n",
      "epoch: 3 iterations: 17000 loss :3.84268e-05\n",
      "epoch: 3 iterations: 17500 loss :1.30112e-06\n",
      "epoch: 3 iterations: 18000 loss :0.000173567\n",
      "epoch: 3 iterations: 18500 loss :0.217863\n",
      "epoch: 3 iterations: 19000 loss :1.5208e-06\n",
      "epoch: 3 iterations: 19500 loss :1.86304e-05\n",
      "epoch: 3 iterations: 20000 loss :1.94424e-05\n",
      "epoch: 3 iterations: 20500 loss :1.72748e-06\n",
      "epoch: 3 iterations: 21000 loss :8.97151e-06\n",
      "epoch: 3 iterations: 21500 loss :5.44883e-07\n",
      "epoch: 3 iterations: 22000 loss :4.88134e-07\n",
      "epoch: 3 iterations: 22500 loss :0.000941133\n",
      "epoch: 3 iterations: 23000 loss :1.48997e-06\n",
      "epoch: 3 iterations: 23500 loss :0.000138561\n",
      "epoch: 3 iterations: 24000 loss :6.4759e-07\n",
      "epoch: 3 iterations: 24500 loss :4.78544e-06\n",
      "epoch: 3 iterations: 25000 loss :1.40061e-07\n",
      "epoch: 3 iterations: 25500 loss :4.32366e-05\n",
      "epoch: 3 done! \n",
      " train avg_loss:0.0160292 , acc:0.994546\n",
      "('now best dev acc:', 0.9486301369863014)\n",
      "dev avg_loss:0.284464 train acc:0.962329\n",
      "test avg_loss:0.203908 train acc:0.960426\n",
      "New Best Dev!!!\n",
      "epoch: 4 start!\n",
      "epoch: 4 iterations: 500 loss :7.49693e-07\n",
      "epoch: 4 iterations: 1000 loss :0.00256831\n",
      "epoch: 4 iterations: 1500 loss :0.0227607\n",
      "epoch: 4 iterations: 2000 loss :1.54295e-07\n",
      "epoch: 4 iterations: 2500 loss :2.04897e-05\n",
      "epoch: 4 iterations: 3000 loss :1.71604e-07\n",
      "epoch: 4 iterations: 3500 loss :2.23509e-06\n",
      "epoch: 4 iterations: 4000 loss :7.8503e-06\n",
      "epoch: 4 iterations: 4500 loss :3.3094e-07\n",
      "epoch: 4 iterations: 5000 loss :2.60049e-06\n",
      "epoch: 4 iterations: 5500 loss :1.60603e-06\n",
      "epoch: 4 iterations: 6000 loss :1.84113e-09\n",
      "epoch: 4 iterations: 6500 loss :4.72961e-09\n",
      "epoch: 4 iterations: 7000 loss :1.82297e-06\n",
      "epoch: 4 iterations: 7500 loss :3.5841e-09\n",
      "epoch: 4 iterations: 8000 loss :1.10385e-08\n",
      "epoch: 4 iterations: 8500 loss :1.89321e-08\n",
      "epoch: 4 iterations: 9000 loss :4.09504e-10\n",
      "epoch: 4 iterations: 9500 loss :1.37988e-08\n",
      "epoch: 4 iterations: 10000 loss :8.55289e-10\n",
      "epoch: 4 iterations: 10500 loss :9.64844e-08\n",
      "epoch: 4 iterations: 11000 loss :3.97913e-05\n",
      "epoch: 4 iterations: 11500 loss :1.98232e-08\n",
      "epoch: 4 iterations: 12000 loss :1.52798e-05\n",
      "epoch: 4 iterations: 12500 loss :3.07666e-07\n",
      "epoch: 4 iterations: 13000 loss :2.00262e-07\n",
      "epoch: 4 iterations: 13500 loss :4.95657e-05\n",
      "epoch: 4 iterations: 14000 loss :0.00617777\n",
      "epoch: 4 iterations: 14500 loss :5.2559e-06\n",
      "epoch: 4 iterations: 15000 loss :4.5969e-07\n",
      "epoch: 4 iterations: 15500 loss :1.67628e-08\n",
      "epoch: 4 iterations: 16000 loss :2.21047e-08\n",
      "epoch: 4 iterations: 16500 loss :3.94242e-05\n",
      "epoch: 4 iterations: 17000 loss :2.41806e-07\n",
      "epoch: 4 iterations: 17500 loss :1.81868e-08\n",
      "epoch: 4 iterations: 18000 loss :5.05854e-07\n",
      "epoch: 4 iterations: 18500 loss :1.91588e-07\n",
      "epoch: 4 iterations: 19000 loss :0.000233923\n",
      "epoch: 4 iterations: 19500 loss :0.000268532\n",
      "epoch: 4 iterations: 20000 loss :1.2149e-07\n",
      "epoch: 4 iterations: 20500 loss :1.58636e-06\n",
      "epoch: 4 iterations: 21000 loss :1.25728e-06\n",
      "epoch: 4 iterations: 21500 loss :1.0375e-07\n",
      "epoch: 4 iterations: 22000 loss :5.15113e-08\n",
      "epoch: 4 iterations: 22500 loss :3.3377e-07\n",
      "epoch: 4 iterations: 23000 loss :1.77287e-07\n",
      "epoch: 4 iterations: 23500 loss :3.40667e-08\n",
      "epoch: 4 iterations: 24000 loss :8.95543e-07\n",
      "epoch: 4 iterations: 24500 loss :1.69119e-07\n",
      "epoch: 4 iterations: 25000 loss :1.08468e-05\n",
      "epoch: 4 iterations: 25500 loss :1.32416e-05\n",
      "epoch: 4 done! \n",
      " train avg_loss:0.0114346 , acc:0.996455\n",
      "('now best dev acc:', 0.9623287671232876)\n",
      "dev avg_loss:0.21972 train acc:0.962329\n",
      "test avg_loss:0.210855 train acc:0.954338\n",
      "epoch: 5 start!\n",
      "epoch: 5 iterations: 500 loss :6.70338e-07\n",
      "epoch: 5 iterations: 1000 loss :0.000154209\n",
      "epoch: 5 iterations: 1500 loss :2.04382e-07\n",
      "epoch: 5 iterations: 2000 loss :1.86055e-07\n",
      "epoch: 5 iterations: 2500 loss :5.84252e-07\n",
      "epoch: 5 iterations: 3000 loss :3.15507e-07\n",
      "epoch: 5 iterations: 3500 loss :4.19711e-08\n",
      "epoch: 5 iterations: 4000 loss :6.97288e-07\n",
      "epoch: 5 iterations: 4500 loss :5.83623e-07\n",
      "epoch: 5 iterations: 5000 loss :8.26906e-09\n",
      "epoch: 5 iterations: 5500 loss :5.0743e-07\n",
      "epoch: 5 iterations: 6000 loss :3.58422e-09\n",
      "epoch: 5 iterations: 6500 loss :2.96183e-05\n",
      "epoch: 5 iterations: 7000 loss :0.0010653\n",
      "epoch: 5 iterations: 7500 loss :8.01628e-08\n",
      "epoch: 5 iterations: 8000 loss :2.07504e-09\n",
      "epoch: 5 iterations: 8500 loss :4.02393e-10\n",
      "epoch: 5 iterations: 9000 loss :3.43129e-09\n",
      "epoch: 5 iterations: 9500 loss :6.55848e-09\n",
      "epoch: 5 iterations: 10000 loss :6.82459e-11\n",
      "epoch: 5 iterations: 10500 loss :5.783e-07\n",
      "epoch: 5 iterations: 11000 loss :4.94054e-07\n",
      "epoch: 5 iterations: 11500 loss :3.82238e-07\n",
      "epoch: 5 iterations: 12000 loss :1.23011e-07\n",
      "epoch: 5 iterations: 12500 loss :8.16811e-09\n",
      "epoch: 5 iterations: 13000 loss :5.18197e-10\n",
      "epoch: 5 iterations: 13500 loss :2.70379e-08\n",
      "epoch: 5 iterations: 14000 loss :7.15085e-10\n",
      "epoch: 5 iterations: 14500 loss :1.48234e-08\n",
      "epoch: 5 iterations: 15000 loss :3.02354e-11\n",
      "epoch: 5 iterations: 15500 loss :0.000127886\n",
      "epoch: 5 iterations: 16000 loss :4.46483e-05\n",
      "epoch: 5 iterations: 16500 loss :7.68482e-05\n",
      "epoch: 5 iterations: 17000 loss :0.0668831\n",
      "epoch: 5 iterations: 17500 loss :4.03397e-09\n",
      "epoch: 5 iterations: 18000 loss :0.000406564\n",
      "epoch: 5 iterations: 18500 loss :8.3368e-07\n",
      "epoch: 5 iterations: 19000 loss :2.03865e-06\n",
      "epoch: 5 iterations: 19500 loss :1.07395e-08\n",
      "epoch: 5 iterations: 20000 loss :9.81828e-11\n",
      "epoch: 5 iterations: 20500 loss :2.21804e-08\n",
      "epoch: 5 iterations: 21000 loss :9.48042e-12\n",
      "epoch: 5 iterations: 21500 loss :3.46699e-10\n",
      "epoch: 5 iterations: 22000 loss :3.81173e-08\n",
      "epoch: 5 iterations: 22500 loss :4.6149e-08\n",
      "epoch: 5 iterations: 23000 loss :4.5254e-08\n",
      "epoch: 5 iterations: 23500 loss :4.94409e-09\n",
      "epoch: 5 iterations: 24000 loss :5.26203e-09\n",
      "epoch: 5 iterations: 24500 loss :3.05077e-09\n",
      "epoch: 5 iterations: 25000 loss :3.11573e-11\n",
      "epoch: 5 iterations: 25500 loss :2.20074e-08\n",
      "epoch: 5 done! \n",
      " train avg_loss:0.00644378 , acc:0.998169\n",
      "('now best dev acc:', 0.9623287671232876)\n",
      "dev avg_loss:0.369566 train acc:0.962329\n",
      "test avg_loss:0.350835 train acc:0.945205\n",
      "epoch: 6 start!\n",
      "epoch: 6 iterations: 500 loss :3.31465e-07\n",
      "epoch: 6 iterations: 1000 loss :8.82721e-08\n",
      "epoch: 6 iterations: 1500 loss :8.77787e-11\n",
      "epoch: 6 iterations: 2000 loss :5.54046e-12\n",
      "epoch: 6 iterations: 2500 loss :1.454e-07\n",
      "epoch: 6 iterations: 3000 loss :2.72937e-09\n",
      "epoch: 6 iterations: 3500 loss :6.1609e-09\n",
      "epoch: 6 iterations: 4000 loss :8.86704e-10\n",
      "epoch: 6 iterations: 4500 loss :4.74714e-11\n",
      "epoch: 6 iterations: 5000 loss :4.64199e-08\n",
      "epoch: 6 iterations: 5500 loss :1.24636e-09\n",
      "epoch: 6 iterations: 6000 loss :6.01235e-08\n",
      "epoch: 6 iterations: 6500 loss :2.85198e-10\n",
      "epoch: 6 iterations: 7000 loss :3.30296e-08\n",
      "epoch: 6 iterations: 7500 loss :1.03182e-08\n",
      "epoch: 6 iterations: 8000 loss :1.46848e-10\n",
      "epoch: 6 iterations: 8500 loss :4.82807e-09\n",
      "epoch: 6 iterations: 9000 loss :8.18065e-09\n",
      "epoch: 6 iterations: 9500 loss :3.66169e-08\n",
      "epoch: 6 iterations: 10000 loss :2.39549e-08\n",
      "epoch: 6 iterations: 10500 loss :1.7458e-06\n",
      "epoch: 6 iterations: 11000 loss :1.61243e-08\n",
      "epoch: 6 iterations: 11500 loss :2.82327e-06\n",
      "epoch: 6 iterations: 12000 loss :1.41202e-06\n",
      "epoch: 6 iterations: 12500 loss :5.41078e-07\n",
      "epoch: 6 iterations: 13000 loss :1.67711e-07\n",
      "epoch: 6 iterations: 13500 loss :4.99785e-10\n",
      "epoch: 6 iterations: 14000 loss :1.26415e-07\n",
      "epoch: 6 iterations: 14500 loss :9.18615e-07\n",
      "epoch: 6 iterations: 15000 loss :1.37668e-12\n",
      "epoch: 6 iterations: 15500 loss :1.21162e-08\n",
      "epoch: 6 iterations: 16000 loss :2.80913e-05\n",
      "epoch: 6 iterations: 16500 loss :1.72154e-06\n",
      "epoch: 6 iterations: 17000 loss :1.40482e-05\n",
      "epoch: 6 iterations: 17500 loss :6.31771e-08\n",
      "epoch: 6 iterations: 18000 loss :2.05549e-10\n",
      "epoch: 6 iterations: 18500 loss :7.03341e-09\n",
      "epoch: 6 iterations: 19000 loss :3.58531e-09\n",
      "epoch: 6 iterations: 19500 loss :4.20807e-10\n",
      "epoch: 6 iterations: 20000 loss :7.98485e-09\n",
      "epoch: 6 iterations: 20500 loss :2.06022e-08\n",
      "epoch: 6 iterations: 21000 loss :2.34107e-09\n",
      "epoch: 6 iterations: 21500 loss :1.3664e-08\n",
      "epoch: 6 iterations: 22000 loss :3.71317e-09\n",
      "epoch: 6 iterations: 22500 loss :4.44896e-10\n",
      "epoch: 6 iterations: 23000 loss :6.01181e-06\n",
      "epoch: 6 iterations: 23500 loss :7.26708e-10\n",
      "epoch: 6 iterations: 24000 loss :6.79622e-09\n",
      "epoch: 6 iterations: 24500 loss :4.49081e-11\n",
      "epoch: 6 iterations: 25000 loss :2.57993e-09\n",
      "epoch: 6 iterations: 25500 loss :1.15834e-10\n",
      "epoch: 6 done! \n",
      " train avg_loss:0.00521818 , acc:0.998636\n",
      "('now best dev acc:', 0.9623287671232876)\n",
      "dev avg_loss:0.367405 train acc:0.952055\n",
      "test avg_loss:0.354903 train acc:0.94825\n",
      "epoch: 7 start!\n",
      "epoch: 7 iterations: 500 loss :1.55002e-06\n",
      "epoch: 7 iterations: 1000 loss :0.000410223\n",
      "epoch: 7 iterations: 1500 loss :2.11005e-08\n",
      "epoch: 7 iterations: 2000 loss :1.20519e-09\n",
      "epoch: 7 iterations: 2500 loss :1.58204e-09\n",
      "epoch: 7 iterations: 3000 loss :2.50688e-08\n",
      "epoch: 7 iterations: 3500 loss :5.04023e-10\n",
      "epoch: 7 iterations: 4000 loss :1.66296e-09\n",
      "epoch: 7 iterations: 4500 loss :1.13499e-10\n",
      "epoch: 7 iterations: 5000 loss :1.29472e-08\n",
      "epoch: 7 iterations: 5500 loss :5.43974e-11\n",
      "epoch: 7 iterations: 6000 loss :1.73479e-11\n",
      "epoch: 7 iterations: 6500 loss :2.49093e-10\n",
      "epoch: 7 iterations: 7000 loss :5.21704e-10\n",
      "epoch: 7 iterations: 7500 loss :2.91539e-05\n",
      "epoch: 7 iterations: 8000 loss :6.20108e-10\n",
      "epoch: 7 iterations: 8500 loss :2.13573e-09\n",
      "epoch: 7 iterations: 9000 loss :2.33609e-11\n",
      "epoch: 7 iterations: 9500 loss :1.85825e-11\n",
      "epoch: 7 iterations: 10000 loss :7.60455e-07\n",
      "epoch: 7 iterations: 10500 loss :6.75003e-10\n",
      "epoch: 7 iterations: 11000 loss :3.26942e-10\n",
      "epoch: 7 iterations: 11500 loss :1.34434e-08\n",
      "epoch: 7 iterations: 12000 loss :7.04503e-12\n",
      "epoch: 7 iterations: 12500 loss :1.95932e-11\n",
      "epoch: 7 iterations: 13000 loss :2.0348e-10\n",
      "epoch: 7 iterations: 13500 loss :2.9209e-09\n",
      "epoch: 7 iterations: 14000 loss :4.61853e-14\n",
      "epoch: 7 iterations: 14500 loss :7.99769e-11\n",
      "epoch: 7 iterations: 15000 loss :1.6086e-10\n",
      "epoch: 7 iterations: 15500 loss :9.75293e-10\n",
      "epoch: 7 iterations: 16000 loss :2.1518e-08\n",
      "epoch: 7 iterations: 16500 loss :1.10827e-11\n",
      "epoch: 7 iterations: 17000 loss :2.96918e-11\n",
      "epoch: 7 iterations: 17500 loss :4.70498e-08\n",
      "epoch: 7 iterations: 18000 loss :3.95344e-10\n",
      "epoch: 7 iterations: 18500 loss :8.67693e-06\n",
      "epoch: 7 iterations: 19000 loss :3.55761e-08\n",
      "epoch: 7 iterations: 19500 loss :2.07931e-08\n",
      "epoch: 7 iterations: 20000 loss :9.44028e-06\n",
      "epoch: 7 iterations: 20500 loss :3.57933e-08\n",
      "epoch: 7 iterations: 21000 loss :4.38852e-08\n",
      "epoch: 7 iterations: 21500 loss :1.49149e-06\n",
      "epoch: 7 iterations: 22000 loss :3.44136e-08"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Define the BiLSTM class################################################################\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, label_size):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,num_layers,bidirectional=True)\n",
    "        self.hidden2label = nn.Linear(2*hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers*2, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(self.num_layers*2, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        x = embeds.view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(y)\n",
    "        return log_probs\n",
    "########################################################################################\n",
    "\n",
    "#Define the helper functions for the accuracy###########################################\n",
    "def get_accuracy(truth, pred):\n",
    "     assert len(truth)==len(pred)\n",
    "     right = 0\n",
    "     for i in range(len(truth)):\n",
    "         if truth[i]==pred[i]:\n",
    "             right += 1.0\n",
    "     return right/len(truth)\n",
    "\n",
    "def evaluate(model, data, loss_function, word_to_ix, label_to_ix, name ='dev'):\n",
    "    model.eval()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "\n",
    "    for sent, label in data:\n",
    "        truth_res.append(label_to_ix[label])\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "        sent = prepare_sequence(sent, word_to_ix)\n",
    "        label = prepare_label(label, label_to_ix)\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res.append(pred_label)\n",
    "        # model.zero_grad() # should I keep this when I am evaluating the model?\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "    avg_loss /= len(data)\n",
    "    acc = get_accuracy(truth_res, pred_res)\n",
    "    print(name + ' avg_loss:%g train acc:%g' % (avg_loss, acc ))\n",
    "    return acc\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "#Define training per epoch###########################################################\n",
    "\n",
    "def train_epoch(model, train_data, loss_function, optimizer, word_to_ix, label_to_ix, i):\n",
    "    model.train()\n",
    "    \n",
    "    avg_loss = 0.0\n",
    "    count = 0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    batch_sent = []\n",
    "\n",
    "    for sent, label in train_data:\n",
    "\n",
    "\n",
    "        truth_res.append(label_to_ix[label])\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "        sent = prepare_sequence(sent, word_to_ix)\n",
    "        label = prepare_label(label, label_to_ix)\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res.append(pred_label)\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "        count += 1\n",
    "        if count % 500 == 0:\n",
    "            print('epoch: %d iterations: %d loss :%g' % (i, count, loss.data[0]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss /= len(train_data)\n",
    "    print('epoch: %d done! \\n train avg_loss:%g , acc:%g'%(i, avg_loss, get_accuracy(truth_res,pred_res)))\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# The main function that you call for training #########################################\n",
    "def train():\n",
    "    model = BiLSTMClassifier(embedding_dim=EMBEDDING_DIM,hidden_dim=HIDDEN_DIM, num_layers = NUM_LAYERS,\n",
    "                           vocab_size=len(word_to_ix),label_size=len(label_to_ix))\n",
    "    best_dev_acc = 0.0\n",
    "    loss_function = nn.NLLLoss()#negative log likelihood loss \n",
    "    optimizer = optim.Adam(model.parameters(),lr = 1e-3)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2)\n",
    "    f = open('train_test.txt','w')\n",
    "    no_up = 0\n",
    "    for i in range(EPOCH):\n",
    "        random.shuffle(train_data)\n",
    "        print('epoch: %d start!' % i)\n",
    "        train_epoch(model, train_data, loss_function, optimizer, word_to_ix, label_to_ix, i)\n",
    "        print('now best dev acc:',best_dev_acc)\n",
    "        dev_acc = evaluate(model,dev_data,loss_function,word_to_ix,label_to_ix,'dev')\n",
    "        #dev_accs.append(dev_acc)\n",
    "        test_acc = evaluate(model, test_data, loss_function, word_to_ix, label_to_ix, 'test')\n",
    "        #test_accs.append(test_acc)\n",
    "        f.write(str(dev_acc)+\",\"+str(test_acc))\n",
    "        if dev_acc > best_dev_acc:\n",
    "            best_dev_acc = dev_acc\n",
    "            os.system('rm mr_best_model_acc_*.model')\n",
    "            print('New Best Dev!!!')\n",
    "            torch.save(model.state_dict(), 'best_models/bilstm_best_model_acc_' + str(int(test_acc*10000)) + '.model')\n",
    "            no_up = 0\n",
    "        else:\n",
    "            no_up += 1\n",
    "            if no_up >= 10:\n",
    "                f.close()\n",
    "                return model, i \n",
    "                exit()\n",
    "    f.close()\n",
    "    return model,i\n",
    "#train~######\n",
    "model, i = train()\n",
    "torch.save(model.state_dict(), 'best_models/bilstm_best_model_Epoch_'+str(i)+'_acc_' + '.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'best_models/bilstm_best_model_Epoch_'+str(i)+'_acc_' + '.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
